# Experiment Configuration Examples
# Copy to experiments.yaml and customize for your needs

experiments:
  # Pilot experiment for testing setup
  pilot:
    name: "Pilot Test"
    description: "Small-scale test to validate infrastructure"
    models: ["llama-3-8b-instruct"]
    samplers: ["top_p", "top_k"]
    tasks: ["creative_writing"]
    samples_per_condition: 10
    seed: 42
    evaluation:
      gpt4_judge: true
      traditional_metrics: true
    output_dir: "data/pilot"
    
  # Quick sampler comparison
  sampler_comparison:
    name: "Sampler Comparison"
    description: "Compare all sampling methods on one model"
    models: ["llama-3-8b-instruct"]
    samplers: ["top_p", "top_k", "min_p", "temperature"]
    tasks: ["creative_writing", "code_generation"]
    samples_per_condition: 25
    seed: 42
    evaluation:
      gpt4_judge: true
      traditional_metrics: true
    output_dir: "data/sampler_comparison"
    
  # Model comparison
  model_comparison:
    name: "Model Comparison"
    description: "Compare different models with best sampler"
    models: ["llama-3-8b-instruct", "mistral-7b-instruct", "qwen-7b-chat"]
    samplers: ["top_p"]  # Use balanced preset
    sampler_presets: ["balanced"]
    tasks: ["creative_writing", "factual_qa", "code_generation"]
    samples_per_condition: 50
    seed: 42
    evaluation:
      gpt4_judge: true
      traditional_metrics: true
    output_dir: "data/model_comparison"
    
  # Full sweep experiment
  full_sweep:
    name: "Complete Sampling Sweep"
    description: "Comprehensive evaluation across all conditions"
    models: ["llama-3-8b-instruct", "mistral-7b-instruct", "qwen-7b-chat"]
    samplers: ["top_p", "top_k", "min_p", "top_n_sigma", "temperature"]
    tasks: ["creative_writing", "code_generation", "factual_qa", "reasoning"]
    samples_per_condition: 42  # ~10,000 total samples
    seed: 42
    evaluation:
      gpt4_judge: true
      traditional_metrics: true
      human_eval_subset: 100  # Random subset for human evaluation
    output_dir: "data/full_sweep"
    parallel_workers: 4
    
  # Custom parameter sweep
  parameter_sweep:
    name: "Temperature Parameter Sweep"
    description: "Detailed temperature analysis"
    models: ["llama-3-8b-instruct"]
    samplers: ["temperature"]
    custom_parameters:
      temperature: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5]
    tasks: ["creative_writing"]
    samples_per_condition: 20
    seed: 42
    evaluation:
      gpt4_judge: true
      traditional_metrics: true
    output_dir: "data/parameter_sweep"

# Default experiment settings
defaults:
  samples_per_condition: 25
  seed: 42
  max_tokens: 2048
  timeout_seconds: 300
  retry_attempts: 3
  
  evaluation:
    gpt4_judge: true
    traditional_metrics: true
    quality_checks: true
    
  output:
    save_raw: true
    save_processed: true
    save_metadata: true
    format: "jsonl"
    
  performance:
    batch_size: 1
    parallel_workers: 2
    gpu_memory_fraction: 0.9
    
# Task-specific configurations
task_configs:
  creative_writing:
    prompt_template: "creative_writing_v1"
    max_tokens: 1000
    evaluation_criteria: ["creativity", "coherence", "engagement"]
    
  code_generation:
    prompt_template: "humaneval"
    max_tokens: 500
    evaluation_criteria: ["correctness", "efficiency", "readability"]
    
  factual_qa:
    prompt_template: "mmlu"
    max_tokens: 200
    evaluation_criteria: ["accuracy", "factuality", "completeness"]
    
  reasoning:
    prompt_template: "gsm8k"
    max_tokens: 800
    evaluation_criteria: ["correctness", "step_by_step", "clarity"] 