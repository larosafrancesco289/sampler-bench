# Test Creative Writing Configuration
# Based on robust creative writing but with only 10 samples for quick testing

benchmark_config:
  name: "Test Creative Writing Benchmark"
  description: "Quick test configuration with 10 samples for pipeline validation"
  
  # Core experimental design - 10 total samples for testing
  experimental_design:
    # 2 prompts selected from robust creative writing for testing
    prompts:
      character_driven:
        text: "You are a creative writer. Your task is to write a complete short story of exactly 300-400 words.\n\nPROMPT: Two strangers meet during a citywide power outage and discover an unexpected connection.\n\nREQUIREMENTS:\n- Exactly 300-400 words (this is critical)\n- Complete story with beginning, middle, and end\n- Focus on character interaction and connection\n- No meta-commentary, explanations, or author notes\n\nWrite the story now. Stop immediately when the story concludes."
        category: "character/dialogue"
        focus: "Character interaction, relationship development, dialogue, emotional connection"
        writing_skills: ["dialogue", "character_voice", "relationship_dynamics", "emotional_authenticity"]
        
      speculative_concept:
        text: "You are a creative writer. Your task is to write a complete short story of exactly 300-400 words.\n\nPROMPT: A character suddenly discovers they can hear other people's thoughts for exactly 24 hours.\n\nREQUIREMENTS:\n- Exactly 300-400 words (this is critical)\n- Complete story with beginning, middle, and end\n- Focus on the character's experience and transformation\n- No meta-commentary, explanations, or author notes\n\nWrite the story now. Stop immediately when the story concludes."
        category: "high-concept/speculative"
        focus: "Premise exploration, creative problem-solving, consequence development, world-building"
        writing_skills: ["premise_development", "creative_logic", "plot_progression", "conceptual_depth"]
    
    # Sample distribution: 5 samplers × 2 prompts × 1 repetition = 10 samples
    repetitions_per_prompt_per_sampler: 1
    total_samples: 10
    samples_per_sampler: 2  # Minimal for testing
    
    # 5 distinct samplers covering the sampling space
    samplers:
      - "model_default"      # Dynamically maps to provider settings (llama_default, mistral_default, etc.)
      - "standard_minp"      # Min-p standard (temp 0.7, min_p 0.2) - universal
      - "creative_minp"      # Min-p creative (temp 1.0, min_p 0.2) - universal  
      - "standard_sigma"     # Sigma standard (temp 1.5, sigma 1.0) - universal
      - "creative_sigma"     # Sigma moderate (temp 1.0, sigma 1.5) - universal

# Evaluation Framework - Judge configurable via .env
evaluation_framework:
  # Multi-judge configuration from environment  
  judge_config:
    multi_judge_enabled: "${MULTI_JUDGE_ENABLED}"  # Enable multi-judge evaluation
    judge_models: "${LLM_JUDGE_MODELS}"           # Comma-separated list of OpenRouter models
    api_key: "${OPENROUTER_API_KEY}"              # OpenRouter API key
    consensus_method: "${JUDGE_CONSENSUS_METHOD}" # How to combine scores (average, weighted_average)
    temperature: 0.2                              # Conservative temperature for consistent judging
    batch_size: 3  # Reduced for multi-judge parallel processing
    
  # Refined evaluation criteria
  criteria:
    narrative_structure:
      weight: 0.30
      description: "Story organization, pacing, and plot coherence"
      rubric:
        excellent: "Clear arc with compelling opening, development, and resolution"
        good: "Solid structure with minor pacing issues"
        average: "Adequate structure but unclear progression"
        poor: "Disjointed or incomplete narrative"
      
    creativity_execution:
      weight: 0.25  
      description: "Creative premise handling and original elements"
      rubric:
        excellent: "Highly original with creative premise exploration"
        good: "Creative elements with good premise development"
        average: "Some creative touches, standard execution"
        poor: "Clichéd or unimaginative approach"
      
    character_voice:
      weight: 0.20
      description: "Character development and authentic voice"
      rubric:
        excellent: "Distinct, believable characters with clear motivations"
        good: "Well-developed characters with consistent voice"
        average: "Adequate characterization, some depth"
        poor: "Flat or inconsistent characters"
      
    prose_quality:
      weight: 0.15
      description: "Writing craft, style, and language use"
      rubric:
        excellent: "Polished prose with strong style and word choice"
        good: "Good writing with minor technical issues"
        average: "Competent writing, straightforward style"
        poor: "Awkward phrasing or technical problems"
      
    engagement:
      weight: 0.10
      description: "Reader interest and emotional impact"
      rubric:
        excellent: "Compelling and emotionally resonant"
        good: "Engaging with good emotional moments"
        average: "Moderately interesting"
        poor: "Bland or unengaging"

# Quality Control
quality_control:
  automated_checks:
    minimum_length: 150  # words
    maximum_length: 600  # words
    prompt_adherence: true
    basic_coherence: true
    
  # Instruction following penalties
  instruction_penalties:
    word_count_penalty:
      enabled: true
      target_range: [300, 400]  # Target word count range
      penalties:
        severe_deviation: 
          threshold: 100  # >100 words off target range
          penalty: -1.5   # Subtract 1.5 points from overall score
        moderate_deviation:
          threshold: 50   # 50-100 words off target range
          penalty: -1.0   # Subtract 1.0 points from overall score
        minor_deviation:
          threshold: 25   # 25-50 words off target range
          penalty: -0.5   # Subtract 0.5 points from overall score
    
    empty_generation_penalty:
      enabled: true
      penalty: -3.0  # Heavy penalty for empty/failed generations
    
    meta_commentary_penalty:
      enabled: true
      detection_patterns:
        - "word count"
        - "let me know"
        - "would you like"
        - "i hope"
        - "feedback"
        - "revise"
        - "(note:"
      penalty: -0.8  # Penalty for meta-commentary
    
  manual_review_triggers:
    - "judge_score_below_3"
    - "extreme_length_deviation"
    - "potential_prompt_miss"

# Performance Optimization
performance_config:
  generation:
    sequential_prompts: true  # Don't batch prompts (speed optimization)
    parallel_samplers: true   # Process samplers in parallel for test run
    timeout_per_sample: 90    # seconds
    
  evaluation:
    batch_judge_requests: 2   # Smaller batch for testing
    cache_enabled: true
    concurrent_evaluations: 2 # Reduced for testing

# Output Configuration
output_config:
  results_structure:
    individual_samples: true
    sampler_summaries: true
    prompt_analysis: true
    overall_rankings: true
    
  statistical_metrics:
    descriptive_stats:
      mean_scores: true
      standard_deviations: true
      median_scores: true
      quartiles: true
      
    inferential_stats:
      confidence_intervals: false    # Disabled for small sample size
      bootstrap_samples: 100         # Reduced for testing
      effect_sizes: false            # Disabled for small sample size
      significance_tests: false      # Disabled for small sample size
      
    reliability_analysis:
      inter_prompt_correlation: true  # Consistency across prompts
      outlier_detection: true         # IQR method for outliers
      sample_adequacy: false          # Disabled for small sample size
      
    cross_validation:
      train_prompts: ["character_driven"]  # 1 prompt for ranking
      holdout_prompt: "speculative_concept"  # 1 prompt for validation
      ranking_stability: false        # Disabled for small sample size
      expected_samplers: ["model_default", "standard_minp", "creative_minp", "creative_sigma", "standard_sigma"]
    
  quality_analysis:
    best_worst_examples: true
    criteria_breakdown: true
    sampler_strengths: true
    prompt_difficulty: true
    
  advanced_analysis:
    sampler_consistency:
      within_sampler_variance: true    # How consistent is each sampler?
      prompt_specific_performance: true # Which samplers work best for which prompts?
      
    quality_distribution:
      score_histograms: true
      normality_tests: false            # Disabled for small sample size
      
    practical_significance:
      minimum_detectable_effect: 0.8   # Minimum Cohen's d we care about (large effect)
      power_analysis: false             # Disabled for small sample size
      
    robustness_checks:
      sensitivity_analysis: false       # Disabled for small sample size
      bootstrap_rankings: false         # Disabled for small sample size

# Reproducibility
reproducibility:
  seeds:
    generation_seed: 42
    evaluation_order_seed: 123
    
  versioning:
    config_version: "test_1.0"
    prompt_set_version: "v1_test"
    
  documentation:
    save_config_snapshot: true
    log_generation_params: true
    record_timing_data: true