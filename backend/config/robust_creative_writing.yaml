# Robust Creative Writing Benchmark Configuration v2.2
# Optimized for quality and practical execution (100 total samples)

benchmark_config:
  name: "Robust Creative Writing Benchmark v2.2"
  description: "Quality-focused evaluation with robust sample size and optimized execution"
  
  # Core experimental design - 100 total samples with high statistical power
  experimental_design:
    # 5 strategically selected prompts representing core creative writing dimensions
    prompts:
      character_driven:
        text: "Write a complete short story (300-400 words) about two strangers who meet during a citywide power outage and discover an unexpected connection. Make sure to include a proper beginning, development, and conclusion."
        category: "character/dialogue"
        focus: "Character interaction, relationship development, dialogue, emotional connection"
        writing_skills: ["dialogue", "character_voice", "relationship_dynamics", "emotional_authenticity"]
        
      speculative_concept:
        text: "Write a complete short tale (300-400 words) about a character who suddenly discovers they can hear other people's thoughts for exactly 24 hours. Include a clear beginning, middle, and satisfying conclusion."
        category: "high-concept/speculative"
        focus: "Premise exploration, creative problem-solving, consequence development, world-building"
        writing_skills: ["premise_development", "creative_logic", "plot_progression", "conceptual_depth"]
        
      atmospheric_narrative:
        text: "Write a complete short story (300-400 words) about a mysterious package that appears on someone's doorstep one rainy evening. Ensure the story has a proper beginning, development, and resolution."
        category: "atmosphere/mystery"
        focus: "Mood establishment, setting description, tension building, atmospheric storytelling"
        writing_skills: ["descriptive_writing", "mood_creation", "pacing", "atmospheric_tension"]
        
      transformation_story:
        text: "Write a complete short story (300-400 words) about someone who wakes up one morning to find they have gained an unusual superpower for just one day. Include a clear beginning, middle, and satisfying conclusion."
        category: "transformation/adventure"
        focus: "Character transformation, creative problem-solving, conflict resolution, personal growth"
        writing_skills: ["character_arc", "conflict_development", "creative_resolution", "pacing"]
        
      memory_narrative:
        text: "Write a complete short story (300-400 words) about a person who discovers an old photograph that brings back a forgotten memory with unexpected consequences. Make sure to include a proper beginning, development, and conclusion."
        category: "memory/emotion"
        focus: "Emotional depth, memory exploration, consequence development, introspective storytelling"
        writing_skills: ["emotional_resonance", "introspection", "consequence_chain", "reflective_narrative"]
    
    # Sample distribution: 5 samplers × 5 prompts × 4 repetitions = 100 samples
    repetitions_per_prompt_per_sampler: 4
    total_samples: 100
    samples_per_sampler: 20  # Robust statistical analysis
    
    # 5 distinct samplers covering the sampling space
    samplers:
      - "model_default"      # Dynamically maps to provider settings (llama_default, mistral_default, etc.)
      - "standard_minp"      # Min-p standard (temp 0.7, min_p 0.2) - universal
      - "creative_minp"      # Min-p creative (temp 1.0, min_p 0.2) - universal  
      - "standard_sigma"     # Sigma standard (temp 1.5, sigma 1.0) - universal
      - "creative_sigma"     # Sigma moderate (temp 1.0, sigma 1.5) - universal

# Evaluation Framework - Judge configurable via .env
evaluation_framework:
  # Multi-judge configuration from environment  
  judge_config:
    multi_judge_enabled: "${MULTI_JUDGE_ENABLED}"  # Enable multi-judge evaluation
    judge_models: "${LLM_JUDGE_MODELS}"           # Comma-separated list of OpenRouter models
    api_key: "${OPENROUTER_API_KEY}"              # OpenRouter API key
    consensus_method: "${JUDGE_CONSENSUS_METHOD}" # How to combine scores (average, weighted_average)
    temperature: 0.2                              # Conservative temperature for consistent judging
    batch_size: 3  # Reduced for multi-judge parallel processing
    
  # Refined evaluation criteria
  criteria:
    narrative_structure:
      weight: 0.30
      description: "Story organization, pacing, and plot coherence"
      rubric:
        excellent: "Clear arc with compelling opening, development, and resolution"
        good: "Solid structure with minor pacing issues"
        average: "Adequate structure but unclear progression"
        poor: "Disjointed or incomplete narrative"
      
    creativity_execution:
      weight: 0.25  
      description: "Creative premise handling and original elements"
      rubric:
        excellent: "Highly original with creative premise exploration"
        good: "Creative elements with good premise development"
        average: "Some creative touches, standard execution"
        poor: "Clichéd or unimaginative approach"
      
    character_voice:
      weight: 0.20
      description: "Character development and authentic voice"
      rubric:
        excellent: "Distinct, believable characters with clear motivations"
        good: "Well-developed characters with consistent voice"
        average: "Adequate characterization, some depth"
        poor: "Flat or inconsistent characters"
      
    prose_quality:
      weight: 0.15
      description: "Writing craft, style, and language use"
      rubric:
        excellent: "Polished prose with strong style and word choice"
        good: "Good writing with minor technical issues"
        average: "Competent writing, straightforward style"
        poor: "Awkward phrasing or technical problems"
      
    engagement:
      weight: 0.10
      description: "Reader interest and emotional impact"
      rubric:
        excellent: "Compelling and emotionally resonant"
        good: "Engaging with good emotional moments"
        average: "Moderately interesting"
        poor: "Bland or unengaging"

# Quality Control
quality_control:
  automated_checks:
    minimum_length: 150  # words
    maximum_length: 600  # words
    prompt_adherence: true
    basic_coherence: true
    
  manual_review_triggers:
    - "judge_score_below_3"
    - "extreme_length_deviation"
    - "potential_prompt_miss"

# Performance Optimization
performance_config:
  generation:
    sequential_prompts: true  # Don't batch prompts (speed optimization)
    parallel_samplers: true   # Process samplers in parallel for 100-sample run
    timeout_per_sample: 90    # seconds
    
  evaluation:
    batch_judge_requests: 5   # Batch judge API calls
    cache_enabled: true
    concurrent_evaluations: 5 # Parallel judge requests (increased for 100 samples)

# Output Configuration
output_config:
  results_structure:
    individual_samples: true
    sampler_summaries: true
    prompt_analysis: true
    overall_rankings: true
    
  statistical_metrics:
    descriptive_stats:
      mean_scores: true
      standard_deviations: true
      median_scores: true
      quartiles: true
      
    inferential_stats:
      confidence_intervals: true    # 95% CI for sampler means
      bootstrap_samples: 1000       # For robust CI estimation
      effect_sizes: true            # Cohen's d between all sampler pairs
      significance_tests: true      # Pairwise t-tests with correction
      
    reliability_analysis:
      inter_prompt_correlation: true  # Consistency across prompts
      outlier_detection: true         # IQR method for outliers
      sample_adequacy: true           # Power analysis for sample size
      
    cross_validation:
      train_prompts: ["character_driven", "speculative_concept", "atmospheric_narrative", "transformation_story"]  # 4 prompts for ranking
      holdout_prompt: "memory_narrative"                    # 1 prompt for validation
      ranking_stability: true        # How stable are sampler rankings?
      expected_samplers: ["model_default", "standard_minp", "creative_minp", "creative_sigma", "standard_sigma"]
    
  quality_analysis:
    best_worst_examples: true
    criteria_breakdown: true
    sampler_strengths: true
    prompt_difficulty: true
    
  advanced_analysis:
    sampler_consistency:
      within_sampler_variance: true    # How consistent is each sampler?
      prompt_specific_performance: true # Which samplers work best for which prompts?
      
    quality_distribution:
      score_histograms: true
      normality_tests: true            # Check if scores are normally distributed
      
    practical_significance:
      minimum_detectable_effect: 0.8   # Minimum Cohen's d we care about (large effect)
      power_analysis: true             # Did we have enough samples?
      
    robustness_checks:
      sensitivity_analysis: true       # How sensitive are results to outliers?
      bootstrap_rankings: true         # Stability of sampler rankings

# Reproducibility
reproducibility:
  seeds:
    generation_seed: 42
    evaluation_order_seed: 123
    
  versioning:
    config_version: "2.0"
    prompt_set_version: "v2_focused"
    
  documentation:
    save_config_snapshot: true
    log_generation_params: true
    record_timing_data: true 